(window["webpackJsonp"]=window["webpackJsonp"]||[]).push([["tema2"],{"033c":function(a,e,s){a.exports=s.p+"img/img-02.139ef907.svg"},"0963":function(a,e,s){a.exports=s.p+"img/img-07.b901b81d.png"},"0ffe":function(a,e,s){a.exports=s.p+"img/icono-03.2bc2a397.svg"},1200:function(a,e,s){a.exports=s.p+"img/img-06.52dd9e59.png"},"13a3":function(a,e,s){a.exports=s.p+"img/icono-17.74ebdcdc.svg"},"143d":function(a,e,s){a.exports=s.p+"img/icono-06.14d5af6f.svg"},"1c02":function(a,e,s){a.exports=s.p+"img/img-08.a8507a87.png"},"1c4e":function(a,e,s){a.exports=s.p+"img/img-04.97740804.png"},"1f35":function(a,e,s){a.exports=s.p+"img/img-05.f3f0dda7.png"},"23a2":function(a,e,s){a.exports=s.p+"img/ico2.d0271687.svg"},"35bc":function(a,e,s){a.exports=s.p+"img/icono-01.5062c655.svg"},4489:function(a,e,s){a.exports=s.p+"img/icono-04.808bfaba.svg"},"57a3":function(a,e,s){a.exports=s.p+"img/ico.e45fad04.svg"},"5ac4":function(a,e,s){a.exports=s.p+"img/icono-01.fb35b698.svg"},6468:function(a,e,s){a.exports=s.p+"img/img-10.129a2f94.png"},"99da":function(a,e,s){a.exports=s.p+"img/icono-05.4f8ed20f.svg"},a034:function(a,e,s){a.exports=s.p+"img/img-01.3d988b9e.png"},bbee:function(a,e,s){a.exports=s.p+"img/img-09.5618a612.png"},c9ec:function(a,e,s){a.exports=s.p+"img/img-03.726f8e70.svg"},df17:function(a,e,s){a.exports=s.p+"img/img-12.ef09befb.png"},fd11:function(a,e,s){"use strict";s.r(e);var t=function(){var a=this,e=a._self._c;return e("div",{staticClass:"curso-main-container pb-3"},[e("BannerInterno"),e("div",{staticClass:"container tarjeta tarjeta--blanca p-4 p-md-5 mb-5"},[a._m(0),a._m(1),a._m(2),a._m(3),e("Separador"),a._m(4),a._m(5),e("div",{staticClass:"tarjeta tarjeta--BG01 p-md-5 p-4 mb-5",attrs:{"data-aos":"fade-right"}},[e("SlyderA",{attrs:{tipo:"b"}},[e("div",{staticClass:"row justify-content-center p-4"},[e("div",{staticClass:"col-lg-5 order-lg-1 order-2"},[e("h4",[a._v("Dispersión de datos")]),e("p",{staticClass:"mb-0"},[a._v("En un espacio de alta dimensión, los puntos de datos se tornan extremadamente dispersos. La distancia entre cualquier par de puntos tiende a ser similar, lo que dificulta que los algoritmos basados en la distancia, como K-NN, puedan agrupar o diferenciar observaciones de manera eficaz. ")])]),e("div",{staticClass:"col-lg-7 col-10 order-lg-2 order-1 mb-lg-0 mb-4"},[e("img",{attrs:{src:s("1c4e"),alt:"Imagen decorativa"}})])]),e("div",{staticClass:"row justify-content-center p-4"},[e("div",{staticClass:"col-lg-5 order-lg-1 order-2"},[e("h4",[a._v("Aumento del costo computacional")]),e("p",{staticClass:"mb-4"},[a._v("Un mayor número de dimensiones implica más parámetros que un modelo debe aprender, lo cual se traduce directamente en tiempos de entrenamiento más prolongados, mayor consumo de memoria y una infraestructura más costosa. ")])]),e("div",{staticClass:"col-lg-7 col-10 order-lg-2 order-1 mb-lg-0 mb-4"},[e("img",{attrs:{src:s("1f35"),alt:"Imagen decorativa"}})])]),e("div",{staticClass:"row justify-content-center p-4"},[e("div",{staticClass:"col-lg-5 order-lg-1 order-2"},[e("h4",[a._v("Riesgo de sobreajuste "),e("em",[a._v("(overfitting)")])]),e("p",{staticClass:"mb-4"},[a._v('Con una gran cantidad de características, es más probable que un modelo aprenda del "ruido" y de las peculiaridades específicas del conjunto de entrenamiento, en lugar de generalizar los patrones subyacentes. El modelo se vuelve excesivamente complejo y funciona muy bien con los datos que ya ha visto, pero falla al predecir sobre datos nuevos. ')])]),e("div",{staticClass:"col-lg-7 col-10 order-lg-2 order-1 mb-lg-0 mb-4"},[e("img",{attrs:{src:s("1200"),alt:"Imagen decorativa"}})])]),e("div",{staticClass:"row justify-content-center p-4"},[e("div",{staticClass:"col-lg-5 order-lg-1 order-2"},[e("h4",[a._v("Multicolinealidad")]),e("p",{staticClass:"mb-4"},[a._v('En muchos conjuntos de datos, algunas características están altamente correlacionadas entre sí (por ejemplo, las variables "altura en metros" y "altura en centímetros"). Esta redundancia no aporta nueva información y puede desestabilizar algunos modelos de aprendizaje automático, como la regresión lineal. ')])]),e("div",{staticClass:"col-lg-7 col-10 order-lg-2 order-1 mb-lg-0 mb-4"},[e("img",{attrs:{src:s("0963"),alt:"Imagen decorativa"}})])])])],1),a._m(6),e("Separador"),a._m(7),e("p",{staticClass:"mb-5",attrs:{"data-aos":"fade-left"}},[a._v("Las técnicas para reducir la dimensionalidad se dividen en dos grandes familias, cada una con su propia filosofía y aplicabilidad.")]),a._m(8),a._m(9),a._m(10),e("p",{staticClass:"mb-5",attrs:{"data-aos":"fade-left"}},[a._v("Evalúan la relevancia de las características, mediante el uso de métricas estadísticas, independientemente del modelo de aprendizaje automático que se empleará posteriormente. Estos métodos son rápidos y eficientes desde el punto de vista computacional y hacen parte de ello:")]),a._m(11),a._m(12),e("p",{staticClass:"mb-5",attrs:{"data-aos":"fade-left"}},[a._v("Estos métodos emplean un algoritmo de aprendizaje automático específico, para evaluar la utilidad de diferentes subconjuntos de características. Consideran la selección de características como un problema de búsqueda, donde cada estado representa un conjunto de variables. Aunque son más precisos que los métodos de filtro, su costo computacional es significativamente mayor e incluyen:")]),e("div",{staticClass:"row justify-content-center mb-5"},[e("div",{staticClass:"col-lg-10"},[e("TabsC",{staticClass:"color-acento-botones mb-5",attrs:{"data-aos":"fade-right"}},[e("div",{staticClass:"BGIMG01 py-lg-5 p-4",attrs:{titulo:"Eliminación recursiva de características (RFE)"}},[e("div",{staticClass:"row justify-content-center"},[e("div",{staticClass:"col-lg-6 col-8 order-lg-1 order-1 mb-lg-0 mb-4"},[e("img",{attrs:{src:s("bbee"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col-lg-6 order-lg-2 order-2"},[e("p",{staticClass:"mb-0"},[a._v("Este es un método iterativo que comienza entrenando un modelo con todas las características disponibles. Posteriormente, se evalúa la importancia de cada característica, por ejemplo, mediante los coeficientes en una regresión o la impureza en un árbol de decisión. La característica menos importante se elimina y el proceso se repite hasta alcanzar el número deseado de características. ")])])])]),e("div",{staticClass:"BGIMG01 py-lg-5 p-4",attrs:{titulo:"Selección hacia adelante <em>(forward selection)</em>"}},[e("div",{staticClass:"row justify-content-center"},[e("div",{staticClass:"col-lg-6 col-8 order-lg-1 order-1 mb-lg-0 mb-4"},[e("figure",[e("img",{attrs:{src:s("6468"),alt:"Imagen decorativa"}})])]),e("div",{staticClass:"col-lg-6 order-lg-2 order-2"},[e("p",{staticClass:"mb-0"},[a._v("Este método inicia sin características y procede a añadirlas una a una, seleccionando aquella que más mejora el rendimiento del modelo, hasta que no se observe una mejora significativa.")])])])])])],1)]),a._m(13),e("p",{staticClass:"mb-5",attrs:{"data-aos":"fade-left"}},[a._v("Realizan la selección de características, como parte integral del proceso de entrenamiento del modelo. Estos métodos ofrecen un equilibrio entre la precisión de los métodos de envoltura y la eficiencia de los métodos de filtro; además, incluyen:")]),e("div",{staticClass:"row justify-content-center mb-5"},[e("div",{staticClass:"col-lg-7 order-lg-1 order-2",attrs:{"data-aos":"fade-right"}},[e("LineaTiempoD",{staticClass:"color-acento-botones especial"},[e("p",{staticClass:"mb-0",attrs:{numero:"1",titulo:"Regularización L1 (Lasso)"}},[a._v("Los modelos como la regresión Lasso, incorporan una penalización basada en la suma del valor absoluto de los coeficientes del modelo. Esta penalización induce a que los coeficientes de las características menos informativas se reduzcan a exactamente cero, eliminándolas efectivamente del modelo (Tibshirani, 1996). ")]),e("p",{staticClass:"mb-0",attrs:{numero:"2",titulo:"Modelos basados en árboles"}},[a._v('Algoritmos como Random Forest o Gradient Boosting, calculan de manera inherente la "importancia de las características" durante su construcción. Esta métrica puede emplearse para clasificar las características y seleccionar las más relevantes. Estos métodos pueden utilizarse para seleccionar las características más relevantes y mejorar el rendimiento del modelo. La importancia de las características en los modelos basados en árboles, se calcula generalmente, mediante la reducción en la impureza (como el índice Gini o la entropía) que proporciona cada característica. Además, estos métodos son robustos frente a características no lineales y pueden capturar interacciones complejas entre variables.')])])],1),a._m(14)]),a._m(15),e("div",{staticClass:"row align-items-center mb-5"},[e("div",{staticClass:"col-xl"},[e("p",[a._v("La segunda técnica que se utiliza es la extracción de características, conocida también como proyección de características; la cual convierte los datos originales en un espacio de menor dimensión al crear nuevas variables que son combinaciones de las originales. Este método es particularmente beneficioso, cuando las características originales presentan alta correlación o son redundantes. Entre las técnicas más destacadas se encuentran:")]),e("AcordionA",{attrs:{tipo:"b","clase-tarjeta":"tarjeta tarjeta--C02"}},[e("div",{attrs:{titulo:"El análisis de componentes principales (PCA)"}},[e("p",{staticClass:"mb-0"},[a._v("Es una de las más empleadas que proyecta los datos en las direcciones de máxima varianza. PCA genera nuevas variables denominadas componentes principales, las cuales son combinaciones lineales de las variables originales. Estos componentes se ordenan, de acuerdo con la cantidad de varianza que explican, lo que permite reducir la dimensionalidad al seleccionar únicamente los componentes más significativos. Con PCA se busca la proyección que mejor representa los datos en términos de mínimos cuadrados. Es particularmente eficaz para la visualización y exploración de conjuntos de datos de alta dimensión, ya que permite identificar fácilmente tendencias, patrones o valores atípicos. Además, reduce la complejidad del modelo y minimiza problemas como la multicolinealidad y el sobreajuste.")])]),e("div",{attrs:{titulo:"El análisis discriminante lineal (LDA)"}},[e("p",{staticClass:"mb-0"},[a._v("A diferencia de PCA se centra en maximizar la separabilidad entre clases. Este es un método de clasificación supervisado en el que se conocen de antemano dos o más grupos, y las nuevas observaciones se clasifican en uno de ellos en función de sus características. LDA estima la probabilidad de que una observación, dado un valor específico de predictores, pertenezca a cada una de las clases de la variable cualitativa.")])]),e("div",{attrs:{titulo:"Métodos no lineales"}},[e("p",{staticClass:"mb-0"},[a._v("Para datos que residen en una variedad no lineal, se emplean técnicas como t-SNE (t-Distributed Stochastic Neighbor Embedding), UMAP (Uniform Manifold Approximation and Projection) y autocodificadores. Estas técnicas son capaces de capturar relaciones más complejas entre las variables en comparación con los métodos lineales como PCA y LDA.")])])])],1),a._m(16)]),a._m(17)],1)],1)},i=[function(){var a=this,e=a._self._c;return e("div",{staticClass:"titulo-principal color-acento-contenido",attrs:{"data-aos":"fade-right"}},[e("div",{staticClass:"titulo-principal__numero"},[e("span",[a._v("2")])]),e("h1",[a._v("Reducción de dimensionalidad")])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row justify-content-center mb-5"},[e("div",{staticClass:"col-lg-8 order-lg-1 order-2",attrs:{"data-aos":"fade-right"}},[e("p",[a._v("Una vez que se han depurado los datos, asegurando su calidad y consistencia, se enfrenta a un nuevo desafío, sutil, pero de gran impacto: la complejidad. En la era del "),e("em",[a._v("Big Data")]),a._v(', es común trabajar con conjuntos de datos que no solo contienen millones de registros (filas), sino también cientos o miles de características (columnas o dimensiones). Aunque podría parecer que "más datos es siempre mejor", un exceso de dimensiones puede ser perjudicial. Este fenómeno, conocido como la "maldición de la dimensionalidad", puede degradar el rendimiento del modelo, aumentar la complejidad computacional y oscurecer los patrones verdaderamente significativos.')]),e("div",{staticClass:"tarjeta BG00 p-4 mb-4"},[e("div",{staticClass:"row justify-content-center align-items-center"},[e("div",{staticClass:"col-lg-2 col-4 mb-lg-0 mb-4"},[e("img",{attrs:{src:s("5ac4"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col-lg-10"},[e("p",{staticClass:"mb-0"},[a._v("La reducción de dimensionalidad constituye un conjunto de técnicas de ingeniería de características empleadas para disminuir el número de variables de entrada en un conjunto de datos, transformándolo en un espacio de menor dimensionalidad, sin perder una cantidad significativa de información relevante. El objetivo no es simplemente eliminar columnas de manera aleatoria, sino hacerlo de una forma inteligente y estructurada que simplifique el problema para los algoritmos de aprendizaje automático. ")])])])])]),e("div",{staticClass:"col-lg-4 col-8 order-lg-2 order-1 mb-lg-0 mb-4",attrs:{"data-aos":"fade-left"}},[e("img",{attrs:{src:s("a034"),alt:"Imagen decorativa"}})])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row justify-content-center mb-4",attrs:{"data-aos":"fade-left"}},[e("div",{staticClass:"col-lg-10"},[e("p",{staticClass:"mb-0"},[a._v("Este proceso no solo mejora la eficiencia computacional, sino que también puede revelar estructuras latentes en los datos que no eran evidentes en su forma original de alta dimensionalidad. ")])])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row justify-content-center bg-1 mb-4 m-20"},[e("div",{staticClass:"col-10"},[e("div",{staticClass:"px-5 py-4"},[e("div",{staticClass:"row justify-content-center align-items-center tarjeta tarjeta--C01 p-3",staticStyle:{"background-color":"#FFEBF1"}},[e("div",{staticClass:"col-lg-8 order-lg-1 order-2 px-4",attrs:{"data-aos":"fade-right"}},[e("div",{staticClass:"row justify-content-center"},[e("p",[a._v("Algunas técnicas populares de reducción de dimensionalidad incluyen:")]),e("ul",{staticClass:"lista-ul--color lista-ul mb-0"},[e("li",{staticClass:"mb-0"},[e("i",{staticClass:"fas fa-robot",staticStyle:{color:"#523DBF"}}),e("p",{staticClass:"mb-0"},[a._v("Análisis de Componentes Principales (PCA).")])]),e("li",{staticClass:"mb-0"},[e("i",{staticClass:"fas fa-robot",staticStyle:{color:"#523DBF"}}),e("p",{staticClass:"mb-0"},[a._v("Análisis de Componentes Independientes (ICA).")])]),e("li",{staticClass:"mb-0"},[e("i",{staticClass:"fas fa-robot",staticStyle:{color:"#523DBF"}}),e("p",{staticClass:"mb-0"},[a._v("Métodos de selección de características.")])])])])]),e("div",{staticClass:"col-lg-4 col-md-6 col-8 order-lg-2 order-1 mb-lg-0 mb-4 px-0"},[e("img",{attrs:{src:s("033c"),alt:"Imagen decorativa"}})])]),e("div",{staticClass:"p-4 bg-c1",attrs:{"data-aos":"fade-left"}},[e("div",{staticClass:"row align-items-center"},[e("div",{staticClass:"col-lg-auto col-3 mb-lg-0 mb-4"},[e("img",{staticClass:"m-auto",staticStyle:{width:"60px"},attrs:{src:s("35bc"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col-lg"},[e("p",{staticClass:"mb-0"},[e("b",[a._v("La elección de la técnica adecuada, dependerá de la naturaleza específica de los datos y los objetivos del análisis, requiriendo una comprensión profunda, tanto del dominio del problema como de las matemáticas subyacentes.")])])])])])])])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"titulo-segundo color-acento-contenido",attrs:{id:"t_2_1","data-aos":"fade-right"}},[e("h2",[a._v("2.1 Concepto")])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row justify-content-center mb-5"},[e("div",{staticClass:"col-lg-9 order-lg-1 order-2",attrs:{"data-aos":"fade-right"}},[e("p",[a._v('La reducción de dimensionalidad se refiere al proceso de transformar un conjunto de datos con un elevado número de variables originales en otro de menor dimensión, preservando al máximo la información relevante. Esta práctica mejora la eficiencia computacional, mitiga la "maldición de la dimensionalidad" y facilita la visualización de datos. (Jolliffe, 2002).')]),e("div",{staticClass:"row justify-content-center align-items-center"},[e("div",{staticClass:"col-lg-1 col-2 px-0 mb-lg-0 mb-4"},[e("img",{attrs:{src:s("13a3"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col-lg-11"},[e("p",{staticClass:"mb-0"},[a._v('El concepto central de la reducción de dimensionalidad se fundamenta en la mitigación de la "maldición de la dimensionalidad", un término acuñado por el matemático Richard Bellman (1961). Este fenómeno describe cómo, al incrementarse el número de dimensiones, el volumen del espacio de características crece de manera exponencial, generando varios efectos adversos: ')])])])]),e("div",{staticClass:"col-lg-3 col-6 order-lg-2 order-1 mb-lg-0 mb-4",attrs:{"data-aos":"fade-left"}},[e("img",{attrs:{src:s("c9ec"),alt:"Imagen decorativa"}})])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row justify-content-center mb-4"},[e("div",{staticClass:"col-lg-4 col-md-6 col-8 mb-lg-0 mb-4"},[e("img",{attrs:{src:s("1c02"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col-lg-8 mb-lg-0 mb-4",attrs:{"data-aos":"fade-right"}},[e("p",{staticClass:"mb-0"},[a._v("La reducción de dimensionalidad aborda estos desafíos al identificar y retener solo las características más relevantes y significativas del conjunto de datos original. Este proceso no solo mitiga los problemas mencionados, sino que también puede revelar estructuras latentes en los datos que no eran evidentes en su forma de alta dimensión. Además, la visualización de datos se vuelve más factible y comprensible cuando se reduce a dos o tres dimensiones, permitiendo a los analistas y científicos de datos obtener "),e("em",[a._v("insights")]),a._v(" valiosos que de otra manera podrían pasar desapercibidos.")])])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"titulo-segundo color-acento-contenido",attrs:{id:"t_2_2","data-aos":"fade-right"}},[e("h2",[a._v("2.2 Técnicas")])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row mb-5 ms-0"},[e("div",{staticClass:"col-auto bg-c2",attrs:{"data-aos":"fade-left"}},[e("div",{staticClass:"row align-items-center"},[e("div",{staticClass:"col-auto px-0"},[e("img",{staticStyle:{width:"40px"},attrs:{src:s("57a3"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col"},[e("h3",{staticClass:"mb-0"},[a._v("A. Selección de características "),e("em",[a._v("(feature selection)")])])])])])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row justify-content-center align-items-center mb-5"},[e("div",{staticClass:"col-lg-2 col-4 px-0 mb-lg-0 mb-4",attrs:{"data-aos":"fade-right"}},[e("img",{attrs:{src:s("0ffe"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col-lg-10",attrs:{"data-aos":"fade-left"}},[e("p",{staticClass:"mb-0"},[a._v("Este enfoque se centra en la identificación y selección de un subconjunto de las características originales, descartando las restantes. La principal ventaja de este método radica en que preserva la interpretabilidad de las variables originales, ya que se retienen únicamente las columnas más informativas. Estas técnicas pueden clasificarse en tres categorías, (Guyon & Elisseeff, 2003):")])])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row mb-5 ms-0"},[e("div",{staticClass:"col-auto bg-c2",attrs:{"data-aos":"fade-left"}},[e("div",{staticClass:"row align-items-center"},[e("div",{staticClass:"col-auto px-0"},[e("img",{staticStyle:{width:"5.7px"},attrs:{src:s("23a2"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col"},[e("h3",{staticClass:"mb-0"},[a._v("I. Métodos de filtro")])])])])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row justify-content-center mb-5",attrs:{"data-aos":"fade-right"}},[e("div",{staticClass:"col-xl-4 col-lg-8 mb-lg-0 mb-4"},[e("div",{staticClass:"crd crd--grayGrad",attrs:{"data-aos":"fade-right"}},[e("div",{staticClass:"bottomCircle mb-4"},[e("figure",[e("img",{staticClass:"img90 m-auto",attrs:{src:s("4489"),alt:"Imagen decorativa"}})])]),e("h4",{staticClass:"titulo-1 text-center"},[a._v("Prueba de chi-cuadrado (X2)")]),e("p",{staticClass:"mb-0"},[a._v("Se emplea para determinar si existe una dependencia significativa entre dos variables categóricas. Se utiliza comúnmente para seleccionar las características categóricas más relevantes para una variable objetiva también categórica. ")])])]),e("div",{staticClass:"col-xl-4 col-lg-8 mb-lg-0 mb-4"},[e("div",{staticClass:"crd crd--grayGrad",attrs:{"data-aos":"flip-up"}},[e("div",{staticClass:"bottomCircle mb-4"},[e("figure",[e("img",{staticClass:"img90 m-auto",attrs:{src:s("99da"),alt:"Imagen decorativa"}})])]),e("h4",{staticClass:"titulo-1"},[a._v("ANOVA (análisis de varianza)")]),e("p",{staticClass:"mb-0"},[a._v("La prueba F de ANOVA permite comparar las medias de una variable continua entre dos o más grupos categóricos. Es útil para seleccionar características numéricas que tienen una relación fuerte con una variable objetiva categórica.")])])]),e("div",{staticClass:"col-xl-4 col-lg-8 mb-lg-0 mb-4"},[e("div",{staticClass:"crd crd--grayGrad",attrs:{"data-aos":"fade-left"}},[e("div",{staticClass:"bottomCircle mb-4"},[e("figure",[e("img",{staticClass:"img90 m-auto",attrs:{src:s("143d"),alt:"Imagen decorativa"}})])]),e("h4",{staticClass:"titulo-1"},[a._v("Coeficiente de correlación de Pearson")]),e("p",{staticClass:"mb-0"},[a._v("Mide la relación lineal entre dos variables numéricas. Se utiliza para identificar características que están altamente correlacionadas con la variable objetivo (en problemas de regresión) y también para identificar y eliminar características redundantes (multicolinealidad).")])])])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row mb-5 ms-0"},[e("div",{staticClass:"col-auto bg-c2",attrs:{"data-aos":"fade-left"}},[e("div",{staticClass:"row align-items-center"},[e("div",{staticClass:"col-auto px-0"},[e("img",{staticStyle:{width:"5.7px"},attrs:{src:s("23a2"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col"},[e("h3",{staticClass:"mb-0"},[a._v("II. Métodos de envoltura "),e("em",[a._v("(wrapper methods)")])])])])])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row mb-5 ms-0"},[e("div",{staticClass:"col-auto bg-c2",attrs:{"data-aos":"fade-left"}},[e("div",{staticClass:"row align-items-center"},[e("div",{staticClass:"col-auto px-0"},[e("img",{staticStyle:{width:"5.7px"},attrs:{src:s("23a2"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col"},[e("h3",{staticClass:"mb-0"},[a._v("III. Métodos integrados "),e("em",[a._v("(embedded methods)")])])])])])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"col-lg-3 col-8 order-lg-2 order-1 mb-lg-0 mb-4",attrs:{"data-aos":"fade-left"}},[e("img",{attrs:{src:s("ff8c"),alt:"Imagen decorativa"}})])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row mb-5 ms-0"},[e("div",{staticClass:"col-auto bg-c2",attrs:{"data-aos":"fade-left"}},[e("div",{staticClass:"row align-items-center"},[e("div",{staticClass:"col-auto px-0"},[e("img",{staticStyle:{width:"40px"},attrs:{src:s("57a3"),alt:"Imagen decorativa"}})]),e("div",{staticClass:"col"},[e("h3",{staticClass:"mb-0"},[a._v("B. Extracción de características")])])])])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"col-xl-auto",attrs:{"data-aos":"fade-left"}},[e("figure",{staticClass:"d-none d-xl-block"},[e("img",{staticClass:"m-auto",attrs:{src:s("df17"),alt:"Imagen decorativa"}})])])},function(){var a=this,e=a._self._c;return e("div",{staticClass:"row justify-content-center mb-4"},[e("div",{staticClass:"col-lg-10",attrs:{"data-aos":"fade-right"}},[e("div",{staticClass:"p-4 bg-c1"},[e("div",{staticClass:"row align-items-center"},[e("div",{staticClass:"col-lg-auto mb-4 mb-lg-0"},[e("figure",[e("img",{staticClass:"m-auto",staticStyle:{width:"60px"},attrs:{src:s("35bc"),alt:"Imagen decorativa"}})])]),e("div",{staticClass:"col-lg col-3"},[e("p",{staticClass:"mb-0"},[e("b",[a._v("La decisión entre la selección y la extracción de características, así como la técnica específica a emplear, dependerá del objetivo final: si la interpretabilidad es fundamental, se prefiere la selección. En cambio, si el objetivo es la máxima compactación de la información, la extracción puede resultar más eficaz.")])])])])])])])}],o={name:"Tema2",data:()=>({}),mounted(){this.$nextTick(()=>{this.$aosRefresh()})},updated(){this.$aosRefresh()}},c=o,n=s("2877"),r=Object(n["a"])(c,t,i,!1,null,null,null);e["default"]=r.exports},ff8c:function(a,e,s){a.exports=s.p+"img/img-11.c51b4ffe.svg"}}]);
//# sourceMappingURL=tema2.cccaf915.js.map